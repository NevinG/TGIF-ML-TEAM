{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How we made the model\n",
    "**Step 1**: Find and scrape data\n",
    "\n",
    "First we found two websites that we were going to scrape to try and get data.\n",
    "- https://capitol.texas.gov\n",
    "- https://lrl.texas.gov\n",
    "\n",
    "There were two main steps to scrape all the necessary data. We needed to get data about individual bills, and we needed to get data about legislative sessions. \n",
    "\n",
    "## Scraping Bill data\n",
    "All bill data was scraped in 'bill_data_scraper.py'. This script is ran through the terminal with 'python bill_data_scraper.py'. You then give it which bill, session, and chamber of congress you want it to start scraping from. This allows you to have multiple instances of the script running, allowing you to scrape the data much faster than just one script.\n",
    "\n",
    "### How The Script Works\n",
    "The script starts by looking up each bill at 'https://capitol.texas.gov/BillLookup/History.aspx?LegSess={}&Bill={}{}' The script goes through all bills in that section, and when it fails to find a bill 10 times, then it moves on to the next chamber or legSess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'continue' not properly in loop (1122528538.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    continue\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'continue' not properly in loop\n"
     ]
    }
   ],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "if soup.text.find('The bill number does not exist for the selected legislative session.') != -1:\n",
    "        if times_in_row_failed_to_find_bill < 10:\n",
    "            times_in_row_failed_to_find_bill+=1\n",
    "            bill_number+=1\n",
    "            continue\n",
    "        \n",
    "        times_in_row_failed_to_find_bill = 0\n",
    "\n",
    "        #save to csv\n",
    "        if df is not None:\n",
    "            end = time.time()\n",
    "            print('session {}, {}, complete through bill {} --  time: {}'.format(leg_sessions[leg_session_index], bill_name, bill_number - 11, end - start))\n",
    "            if not path.isfile('bill_data.csv'):\n",
    "                df.to_csv('bill_data.csv', mode='a',index=False)\n",
    "            else:\n",
    "                df.to_csv('bill_data.csv', mode='a',index=False, header=False)\n",
    "            df = None\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "        if bill_name == 'HB':\n",
    "            bill_name = 'SB'\n",
    "            bill_number = 1\n",
    "\n",
    "        else:\n",
    "            leg_session_index+=1\n",
    "            bill_number = 1\n",
    "            bill_name = 'HB'\n",
    "\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we could find the bill data, we use requests and beautiful soup to extract the data from each bill. Requests allows us to get the HTML from the page, and beautiful makes it easy to parse through all the data and extract what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "URL = 'https://capitol.texas.gov/BillLookup/History.aspx?LegSess={}&Bill={}{}'.format(leg_sessions[leg_session_index],bill_name,bill_number)\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up scraping data from :\n",
    "- https://capitol.texas.gov/BillLookup/History.aspx?LegSess={}&Bill={}{}\n",
    "- https://capitol.texas.gov/BillLookup/Authors.aspx?LegSess={}&Bill={}{}\n",
    "- https://capitol.texas.gov/BillLookup/Actions.aspx?LegSess={}&Bill={}{}\n",
    "\n",
    "\n",
    "All bill data was stored in a dictionary named 'bill'. The code below is an example on how we scraped data from the history page, this process is repeated from all the links above, which in total gives us all the data we scraped from https://capitol.texas.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "#get bill name details\n",
    "billName = soup.find(id=\"usrBillInfoTabs_lblBill\").text\n",
    "bill[\"house_bill\"] = billName.find(\"HB\") != -1\n",
    "bill[\"senate_bill\"] = billName.find(\"SB\") != -1\n",
    "bill[\"bill_number\"] = int(billName[3:])\n",
    "\n",
    "#get bill legislative session\n",
    "bill[\"legislative_session\"] = leg_sessions[leg_session_index]\n",
    "\n",
    "#get house committee\n",
    "house_committee = soup.find(id=\"cellComm1Committee\")\n",
    "if house_committee:\n",
    "    bill[\"house_committee\"] = house_committee.text\n",
    "\n",
    "#get senate committee\n",
    "senate_committee = soup.find(id=\"cellComm2Committee\")\n",
    "if senate_committee:\n",
    "    bill[\"senate_committee\"] = senate_committee.text\n",
    "\n",
    "#get subjects\n",
    "subjects = soup.find(id=\"cellSubjects\")\n",
    "if subjects:\n",
    "    bill[\"subjects\"] = subjects.contents[::2]\n",
    "\n",
    "#get sponsor\n",
    "sponsor = soup.find(id=\"cellSponsors\")\n",
    "if sponsor:\n",
    "    bill[\"sponsor\"] = sponsor.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we saved the data\n",
    "All the bill data was saved into 'bill_data.csv'. This csv is just a huge excel sheet of all the data about each bill. Before the data is saved in the spreadsheet some data is concatenated from 'leg_sess_data.csv', but I'll go how we got that data next. Once the data about the legSess is added the following shows how each bill is added as a row in a dataframe, and then how that dataframe is added to the end of the csv. Data is added to the end of the csv after 100 bills, this way the program isn't slowed down by writing to the csv after every bill, but still saves incrementaly incase there was a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "\n",
    "#add to df\n",
    "if df is None:\n",
    "    df = pd.DataFrame(np.array([list(bill_data_for_csv.values())]), columns=list(bill_data_for_csv.keys()))\n",
    "else:\n",
    "    df2 = pd.DataFrame(np.array([list(bill_data_for_csv.values())]), columns=list(bill_data_for_csv.keys()))\n",
    "    df = pd.concat([df, df2])\n",
    "\n",
    "\n",
    "\n",
    "#save to csv\n",
    "if df is not None:\n",
    "    if bill_number % 100 == 0:\n",
    "        end = time.time()\n",
    "        print('session {}, {}, {} - {} complete  --  time: {}'.format(leg_sessions[leg_session_index], bill_name, bill_number - 100, bill_number, end - start))\n",
    "        if not path.isfile('bill_data.csv'):\n",
    "            df.to_csv('bill_data.csv', mode='a',index=False)\n",
    "        else:\n",
    "            df.to_csv('bill_data.csv', mode='a',index=False, header=False)\n",
    "        df = None\n",
    "        start = time.time()\n",
    "\n",
    "bill_number+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Legislative Session Data\n",
    "\n",
    "All legislative session data was scraped from 'https://lrl.texas.gov' and saved into 'leg_sess_data.csv'. 'bill_data_scraper.py' used 'leg_sess_data.csv' to complete its information about each bill.\n",
    "\n",
    "Every bill introduced into the texas legislature came from a legislative session. These legislative sessions have a certain number of democrats, republicans, young congressmen/women, and old congressmen/women. 'legislative_session_data_scraper' is the script that scraped the data from 'https://lrl.texas.gov' and saved into 'leg_sess_data.csv'.\n",
    "\n",
    "\n",
    "### How the Script Works\n",
    "Once again we use requests and beautiful soup to parse html pages and gain information about the legislative session.\n",
    "The exact pages used to extract data are:\n",
    "- 'https://lrl.texas.gov/sessions/memberStatistics.cfm'\n",
    "- 'https://lrl.texas.gov/committees/cmtes.cfm?from=session&session={}'\n",
    "- 'https://lrl.texas.gov/'\n",
    "- 'https://lrl.texas.gov/committees/{}'\n",
    "\n",
    "The script reads from general summary pages and also reads through committe pages, then looks at each committee members politcal party and aggregates all that data into 'leg_sess_data.csv'\n",
    "\n",
    "## Combinging Legislative Session Data into 'bill_data_scraper.py'\n",
    "\n",
    "The legSess data from 'leg_sess_data.csv' is combined will individual bill information in 'bill_data_scraper.py' through the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "#get session information\n",
    "leg_sess_df = pd.read_csv('leg_sess_data.csv')\n",
    "session_data = leg_sess_df.loc[leg_sess_df['leg_sess'] == int(leg_sessions[leg_session_index][:2])]\n",
    "\n",
    "bill[\"male_house_members\"] = int(session_data.iloc[0]['male_house_members'])\n",
    "bill[\"male_senate_members\"] = int(session_data.iloc[0]['male_senate_members'])\n",
    "bill[\"female_house_members\"] = int(session_data.iloc[0]['female_house_members'])\n",
    "bill[\"female_senate_members\"] = int(session_data.iloc[0]['female_senate_members'])\n",
    "bill[\"democrat_house_members\"] = int(session_data.iloc[0]['democrat_house_members'])\n",
    "bill[\"democrat_senate_members\"] = int(session_data.iloc[0]['democrat_senate_members'])\n",
    "bill[\"republican_house_members\"] = int(session_data.iloc[0]['republican_house_members'])\n",
    "bill[\"republican_senate_members\"] = int(session_data.iloc[0]['republican_senate_members'])\n",
    "bill[\"house_incumbents\"] = int(session_data.iloc[0]['house_incumbents'])\n",
    "bill[\"senate_incumbents\"] = int(session_data.iloc[0]['senate_incumbents'])\n",
    "bill[\"house_freshman\"] = int(session_data.iloc[0]['house_freshman'])\n",
    "bill[\"senate_freshman\"] = int(session_data.iloc[0]['senate_freshman'])\n",
    "bill[\"house_members_age_under_30\"] = int(session_data.iloc[0]['house_members_age_under_30'])\n",
    "bill[\"senate_members_age_under_30\"] = int(session_data.iloc[0]['senate_members_age_under_30'])\n",
    "bill[\"house_members_age_30_to_39\"] = int(session_data.iloc[0]['house_members_age_30_to_39'])\n",
    "bill[\"senate_members_age_30_to_39\"] = int(session_data.iloc[0]['senate_members_age_30_to_39'])\n",
    "bill[\"house_members_age_40_to_49\"] = int(session_data.iloc[0]['house_members_age_40_to_49'])\n",
    "bill[\"senate_members_age_40_to_49\"] = int(session_data.iloc[0]['senate_members_age_40_to_49'])\n",
    "bill[\"house_members_age_50_to_59\"] = int(session_data.iloc[0]['house_members_age_50_to_59'])\n",
    "bill[\"senate_members_age_50_to_59\"] = int(session_data.iloc[0]['senate_members_age_50_to_59'])\n",
    "bill[\"house_members_age_60_to_69\"] = int(session_data.iloc[0]['senate_members_age_60_to_69'])\n",
    "bill[\"house_members_age_over_70\"] = int(session_data.iloc[0]['house_members_age_over_70'])\n",
    "bill[\"senate_members_age_over_70\"] = int(session_data.iloc[0]['senate_members_age_over_70'])\n",
    "\n",
    "bill[\"house_committee_democrats\"] = int(session_data.iloc[0]['committee_{}_democrats'.format(committee_to_number.get(bill[\"house_committee\"],0))])\n",
    "bill[\"house_committee_republicans\"] = int(session_data.iloc[0]['committee_{}_republicans'.format(committee_to_number.get(bill[\"house_committee\"],0))])\n",
    "bill[\"senate_committee_democrats\"] = int(session_data.iloc[0]['committee_{}_democrats'.format(committee_to_number.get(bill[\"senate_committee\"],0))])\n",
    "bill[\"senate_committee_republicans\"] = int(session_data.iloc[0]['committee_{}_republicans'.format(committee_to_number.get(bill[\"senate_committee\"],0))])\n",
    "\n",
    "\n",
    "#translate data to what were storing in the csv\n",
    "bill_data_for_csv = {\n",
    "    \"house_bill\": bill[\"house_bill\"],\n",
    "    \"senate_bill\": bill[\"senate_bill\"],\n",
    "    \"bill_number\": bill[\"bill_number\"],\n",
    "    \"legislative_session\": bill[\"legislative_session\"],\n",
    "    #\"party_of_primary_author\": None,\n",
    "    \"num_of_joint_authors\": len(bill['joint_authors']),\n",
    "    \"num_of_co_authors\": len(bill[\"co_authors\"]),\n",
    "    \"num_of_subjects\": len(bill[\"subjects\"]),\n",
    "    #\"party_of_sponsor\": None,\n",
    "    \"passed\": bill[\"passed\"],\n",
    "    \"male_house_members\": bill[\"male_house_members\"],\n",
    "    \"male_senate_members\": bill[\"male_senate_members\"],\n",
    "    \"female_house_members\": bill[\"female_house_members\"],\n",
    "    \"female_senate_members\": bill[\"female_senate_members\"],\n",
    "    \"democrat_house_members\": bill[\"democrat_house_members\"],\n",
    "    \"democrat_senate_members\": bill[\"democrat_senate_members\"],\n",
    "    \"republican_house_members\": bill[\"republican_house_members\"],\n",
    "    \"republican_senate_members\": bill[\"republican_senate_members\"],\n",
    "    \"house_incumbents\": bill[\"house_incumbents\"],\n",
    "    \"senate_incumbents\": bill[\"senate_incumbents\"],\n",
    "    \"house_freshman\": bill[\"house_freshman\"],\n",
    "    \"senate_freshman\": bill[\"senate_freshman\"],\n",
    "    \"house_members_age_under_30\": bill[\"house_members_age_under_30\"],\n",
    "    \"senate_members_age_under_30\": bill[\"senate_members_age_under_30\"],\n",
    "    \"house_members_age_30_to_39\": bill[\"house_members_age_30_to_39\"],\n",
    "    \"senate_members_age_30_to_39\": bill[\"senate_members_age_30_to_39\"],\n",
    "    \"house_members_age_40_to_49\": bill[\"house_members_age_40_to_49\"],\n",
    "    \"senate_members_age_40_to_49\": bill[\"senate_members_age_40_to_49\"],\n",
    "    \"house_members_age_50_to_59\": bill[\"house_members_age_50_to_59\"],\n",
    "    \"senate_members_age_50_to_59\": bill[\"senate_members_age_50_to_59\"],\n",
    "    \"house_members_age_60_to_69\": bill[\"house_members_age_60_to_69\"],\n",
    "    \"house_members_age_over_70\": bill[\"house_members_age_over_70\"],\n",
    "    \"senate_members_age_over_70\": bill[\"senate_members_age_over_70\"],\n",
    "    \"house_committee_democrats\": bill[\"house_committee_democrats\"],\n",
    "    \"house_committee_republicans\": bill[\"house_committee_republicans\"],\n",
    "    \"senate_committee_democrats\": bill[\"senate_committee_democrats\"],\n",
    "    \"senate_committee_republicans\": bill[\"senate_committee_republicans\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Committee Names Debacle\n",
    "\n",
    "If you read through the code you might be wondering about 'committee_dict.py', 'committee_mapping.py', and 'committee_names.txt'. These exist because in https://capitol.texas.gov and https://lrl.texas.gov the committee names are slightly different. For example the same committee might be called 'Appropriations - S/C on Budget Transparency and Reform' on one website, and 'Appropriations Subcommittee on Budget Transparency and Reform' on the other. My solution to this was to assign each committe a number, and then assign each committee name to the respective number. So 'Appropriations - S/C on Budget Transparency and Reform' and 'Appropriations Subcommittee on Budget Transparency and Reform' both represent committee 410.\n",
    "\n",
    "To make this mapping from the different websites I used the scripts and files 'committee_mapping.py', 'committee_names.txt', and 'commitee_dict.py'. You can read through those scripts and hopefully everything makes since.\n",
    "\n",
    "## The Overall Data\n",
    "\n",
    "Overall the data for each bill that was scraped using the above method are:\n",
    "- house_bill\n",
    "- senate_bill\n",
    "- bill_number\n",
    "- legislative_session\n",
    "- num_of_joint_authors\n",
    "- num_of_co_authors\n",
    "- num_of_subjects\n",
    "- passed\n",
    "- male_house_members\n",
    "- male_senate_members\n",
    "- female_house_members\n",
    "- female_senate_members\n",
    "- democrat_house_members\n",
    "- democrat_senate_members\n",
    "- republican_house_members\n",
    "- republican_senate_members\n",
    "- house_incumbents\n",
    "- senate_incumbents\n",
    "- house_freshman\n",
    "- senate_freshman\n",
    "- house_members_age_under_30\n",
    "- senate_members_age_under_30\n",
    "- house_members_age_30_to_39\n",
    "- senate_members_age_30_to_39\n",
    "- house_members_age_40_to_49\n",
    "- senate_members_age_40_to_49\n",
    "- house_members_age_50_to_59\n",
    "- senate_members_age_50_to_59\n",
    "- house_members_age_60_to_69\n",
    "- house_members_age_over_70\n",
    "- senate_members_age_over_70\n",
    "- house_committee_democrats\n",
    "- house_committee_republicans\n",
    "- senate_committee_democrats\n",
    "- senate_committee_republicans\n",
    "\n",
    "### Cleaning up the data and csv\n",
    "\n",
    "Because 'bill_data_scraper.py' can have multiple instances running at a time, the 'bill_data.csv' isn't organized in order. In order to sort this 'bill_data_cleanup.ipynb' was created. By running this script all that happens is it sorts the csv by name so it was easier to tell what data needed to be mined next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('bill_data.csv')\n",
    "\n",
    "df = df.sort_values(by=['legislative_session', 'senate_bill', 'bill_number'])\n",
    "df.to_csv('bill_data.csv', mode='w',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more script was used to clean up the data, 'removing_errors_from_data.ipynb'. There was an issue with house bills have senate committee data, and senate bills having house committee data. This data exists, but we don't want to use it because a bill with senate and house committee data must have passed at least one chamber of congress, meaning it is extremely likely to pass. The first model was way to accurate because of this, so 'removing_errors_from_data.ipynb' fixes this problem for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Now that we have all the data we just need to build the model. The model was built using the data in bill_data.csv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff3068ec3b5457f7e605372206c0d59b94a13e1a25dc92058a9c23507c2ac47b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
