{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How we made the model\n",
    "**Step 1**: Find and scrape data\n",
    "\n",
    "First we found two websites that we were going to scrape to try and get data.\n",
    "- https://capitol.texas.gov\n",
    "- https://lrl.texas.gov\n",
    "\n",
    "There were two main steps to scrape all the necessary data. We needed to get data about individual bills, and we needed to get data about legislative sessions. \n",
    "\n",
    "## Scraping Bill data\n",
    "All bill data was scraped in 'bill_data_scraper.py'. This script is ran through the terminal with 'python bill_data_scraper.py'. You then give it which bill, session, and chamber of congress you want it to start scraping from. This allows you to have multiple instances of the script running, allowing you to scrape the data much faster than just one script.\n",
    "\n",
    "### How The Script Works\n",
    "The script starts by looking up each bill at 'https://capitol.texas.gov/BillLookup/History.aspx?LegSess={}&Bill={}{}' The script goes through all bills in that section, and when it fails to find a bill 10 times, then it moves on to the next chamber or legSess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'continue' not properly in loop (1122528538.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    continue\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'continue' not properly in loop\n"
     ]
    }
   ],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "if soup.text.find('The bill number does not exist for the selected legislative session.') != -1:\n",
    "        if times_in_row_failed_to_find_bill < 10:\n",
    "            times_in_row_failed_to_find_bill+=1\n",
    "            bill_number+=1\n",
    "            continue\n",
    "        \n",
    "        times_in_row_failed_to_find_bill = 0\n",
    "\n",
    "        #save to csv\n",
    "        if df is not None:\n",
    "            end = time.time()\n",
    "            print('session {}, {}, complete through bill {} --  time: {}'.format(leg_sessions[leg_session_index], bill_name, bill_number - 11, end - start))\n",
    "            if not path.isfile('bill_data.csv'):\n",
    "                df.to_csv('bill_data.csv', mode='a',index=False)\n",
    "            else:\n",
    "                df.to_csv('bill_data.csv', mode='a',index=False, header=False)\n",
    "            df = None\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "        if bill_name == 'HB':\n",
    "            bill_name = 'SB'\n",
    "            bill_number = 1\n",
    "\n",
    "        else:\n",
    "            leg_session_index+=1\n",
    "            bill_number = 1\n",
    "            bill_name = 'HB'\n",
    "\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we could find the bill data, we use requests and beautiful soup to extract the data from each bill. Requests allows us to get the HTML from the page, and beautiful makes it easy to parse through all the data and extract what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "URL = 'https://capitol.texas.gov/BillLookup/History.aspx?LegSess={}&Bill={}{}'.format(leg_sessions[leg_session_index],bill_name,bill_number)\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up scraping data from :\n",
    "- https://capitol.texas.gov/BillLookup/History.aspx?LegSess={}&Bill={}{}\n",
    "- https://capitol.texas.gov/BillLookup/Authors.aspx?LegSess={}&Bill={}{}\n",
    "- https://capitol.texas.gov/BillLookup/Actions.aspx?LegSess={}&Bill={}{}\n",
    "\n",
    "\n",
    "All bill data was stored in a dictionary named 'bill'. The code below is an example on how we scraped data from the history page, this process is repeated from all the links above, which in total gives us all the data we scraped from https://capitol.texas.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "#get bill name details\n",
    "billName = soup.find(id=\"usrBillInfoTabs_lblBill\").text\n",
    "bill[\"house_bill\"] = billName.find(\"HB\") != -1\n",
    "bill[\"senate_bill\"] = billName.find(\"SB\") != -1\n",
    "bill[\"bill_number\"] = int(billName[3:])\n",
    "\n",
    "#get bill legislative session\n",
    "bill[\"legislative_session\"] = leg_sessions[leg_session_index]\n",
    "\n",
    "#get house committee\n",
    "house_committee = soup.find(id=\"cellComm1Committee\")\n",
    "if house_committee:\n",
    "    bill[\"house_committee\"] = house_committee.text\n",
    "\n",
    "#get senate committee\n",
    "senate_committee = soup.find(id=\"cellComm2Committee\")\n",
    "if senate_committee:\n",
    "    bill[\"senate_committee\"] = senate_committee.text\n",
    "\n",
    "#get subjects\n",
    "subjects = soup.find(id=\"cellSubjects\")\n",
    "if subjects:\n",
    "    bill[\"subjects\"] = subjects.contents[::2]\n",
    "\n",
    "#get sponsor\n",
    "sponsor = soup.find(id=\"cellSponsors\")\n",
    "if sponsor:\n",
    "    bill[\"sponsor\"] = sponsor.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we saved the data\n",
    "All the bill data was saved into 'bill_data.csv'. This csv is just a huge excel sheet of all the data about each bill. Before the data is saved in the spreadsheet some data is concatenated from 'leg_sess_data.csv', but I'll go how we got that data next. Once the data about the legSess is added the following shows how each bill is added as a row in a dataframe, and then how that dataframe is added to the end of the csv. Data is added to the end of the csv after 100 bills, this way the program isn't slowed down by writing to the csv after every bill, but still saves incrementaly incase there was a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "\n",
    "#add to df\n",
    "if df is None:\n",
    "    df = pd.DataFrame(np.array([list(bill_data_for_csv.values())]), columns=list(bill_data_for_csv.keys()))\n",
    "else:\n",
    "    df2 = pd.DataFrame(np.array([list(bill_data_for_csv.values())]), columns=list(bill_data_for_csv.keys()))\n",
    "    df = pd.concat([df, df2])\n",
    "\n",
    "\n",
    "\n",
    "#save to csv\n",
    "if df is not None:\n",
    "    if bill_number % 100 == 0:\n",
    "        end = time.time()\n",
    "        print('session {}, {}, {} - {} complete  --  time: {}'.format(leg_sessions[leg_session_index], bill_name, bill_number - 100, bill_number, end - start))\n",
    "        if not path.isfile('bill_data.csv'):\n",
    "            df.to_csv('bill_data.csv', mode='a',index=False)\n",
    "        else:\n",
    "            df.to_csv('bill_data.csv', mode='a',index=False, header=False)\n",
    "        df = None\n",
    "        start = time.time()\n",
    "\n",
    "bill_number+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Legislative Session Data\n",
    "\n",
    "All legislative session data was scraped from 'https://lrl.texas.gov' and saved into 'leg_sess_data.csv'. 'bill_data_scraper.py' used 'leg_sess_data.csv' to complete its information about each bill.\n",
    "\n",
    "Every bill introduced into the texas legislature came from a legislative session. These legislative sessions have a certain number of democrats, republicans, young congressmen/women, and old congressmen/women. 'legislative_session_data_scraper' is the script that scraped the data from 'https://lrl.texas.gov' and saved into 'leg_sess_data.csv'.\n",
    "\n",
    "\n",
    "### How the Script Works\n",
    "Once again we use requests and beautiful soup to parse html pages and gain information about the legislative session.\n",
    "The exact pages used to extract data are:\n",
    "- 'https://lrl.texas.gov/sessions/memberStatistics.cfm'\n",
    "- 'https://lrl.texas.gov/committees/cmtes.cfm?from=session&session={}'\n",
    "- 'https://lrl.texas.gov/'\n",
    "- 'https://lrl.texas.gov/committees/{}'\n",
    "\n",
    "The script reads from general summary pages and also reads through committe pages, then looks at each committee members politcal party and aggregates all that data into 'leg_sess_data.csv'\n",
    "\n",
    "## Combinging Legislative Session Data into 'bill_data_scraper.py'\n",
    "\n",
    "The legSess data from 'leg_sess_data.csv' is combined will individual bill information in 'bill_data_scraper.py' through the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "#get session information\n",
    "leg_sess_df = pd.read_csv('leg_sess_data.csv')\n",
    "session_data = leg_sess_df.loc[leg_sess_df['leg_sess'] == int(leg_sessions[leg_session_index][:2])]\n",
    "\n",
    "bill[\"male_house_members\"] = int(session_data.iloc[0]['male_house_members'])\n",
    "bill[\"male_senate_members\"] = int(session_data.iloc[0]['male_senate_members'])\n",
    "bill[\"female_house_members\"] = int(session_data.iloc[0]['female_house_members'])\n",
    "bill[\"female_senate_members\"] = int(session_data.iloc[0]['female_senate_members'])\n",
    "bill[\"democrat_house_members\"] = int(session_data.iloc[0]['democrat_house_members'])\n",
    "bill[\"democrat_senate_members\"] = int(session_data.iloc[0]['democrat_senate_members'])\n",
    "bill[\"republican_house_members\"] = int(session_data.iloc[0]['republican_house_members'])\n",
    "bill[\"republican_senate_members\"] = int(session_data.iloc[0]['republican_senate_members'])\n",
    "bill[\"house_incumbents\"] = int(session_data.iloc[0]['house_incumbents'])\n",
    "bill[\"senate_incumbents\"] = int(session_data.iloc[0]['senate_incumbents'])\n",
    "bill[\"house_freshman\"] = int(session_data.iloc[0]['house_freshman'])\n",
    "bill[\"senate_freshman\"] = int(session_data.iloc[0]['senate_freshman'])\n",
    "bill[\"house_members_age_under_30\"] = int(session_data.iloc[0]['house_members_age_under_30'])\n",
    "bill[\"senate_members_age_under_30\"] = int(session_data.iloc[0]['senate_members_age_under_30'])\n",
    "bill[\"house_members_age_30_to_39\"] = int(session_data.iloc[0]['house_members_age_30_to_39'])\n",
    "bill[\"senate_members_age_30_to_39\"] = int(session_data.iloc[0]['senate_members_age_30_to_39'])\n",
    "bill[\"house_members_age_40_to_49\"] = int(session_data.iloc[0]['house_members_age_40_to_49'])\n",
    "bill[\"senate_members_age_40_to_49\"] = int(session_data.iloc[0]['senate_members_age_40_to_49'])\n",
    "bill[\"house_members_age_50_to_59\"] = int(session_data.iloc[0]['house_members_age_50_to_59'])\n",
    "bill[\"senate_members_age_50_to_59\"] = int(session_data.iloc[0]['senate_members_age_50_to_59'])\n",
    "bill[\"house_members_age_60_to_69\"] = int(session_data.iloc[0]['senate_members_age_60_to_69'])\n",
    "bill[\"house_members_age_over_70\"] = int(session_data.iloc[0]['house_members_age_over_70'])\n",
    "bill[\"senate_members_age_over_70\"] = int(session_data.iloc[0]['senate_members_age_over_70'])\n",
    "\n",
    "bill[\"house_committee_democrats\"] = int(session_data.iloc[0]['committee_{}_democrats'.format(committee_to_number.get(bill[\"house_committee\"],0))])\n",
    "bill[\"house_committee_republicans\"] = int(session_data.iloc[0]['committee_{}_republicans'.format(committee_to_number.get(bill[\"house_committee\"],0))])\n",
    "bill[\"senate_committee_democrats\"] = int(session_data.iloc[0]['committee_{}_democrats'.format(committee_to_number.get(bill[\"senate_committee\"],0))])\n",
    "bill[\"senate_committee_republicans\"] = int(session_data.iloc[0]['committee_{}_republicans'.format(committee_to_number.get(bill[\"senate_committee\"],0))])\n",
    "\n",
    "\n",
    "#translate data to what were storing in the csv\n",
    "bill_data_for_csv = {\n",
    "    \"house_bill\": bill[\"house_bill\"],\n",
    "    \"senate_bill\": bill[\"senate_bill\"],\n",
    "    \"bill_number\": bill[\"bill_number\"],\n",
    "    \"legislative_session\": bill[\"legislative_session\"],\n",
    "    #\"party_of_primary_author\": None,\n",
    "    \"num_of_joint_authors\": len(bill['joint_authors']),\n",
    "    \"num_of_co_authors\": len(bill[\"co_authors\"]),\n",
    "    \"num_of_subjects\": len(bill[\"subjects\"]),\n",
    "    #\"party_of_sponsor\": None,\n",
    "    \"passed\": bill[\"passed\"],\n",
    "    \"male_house_members\": bill[\"male_house_members\"],\n",
    "    \"male_senate_members\": bill[\"male_senate_members\"],\n",
    "    \"female_house_members\": bill[\"female_house_members\"],\n",
    "    \"female_senate_members\": bill[\"female_senate_members\"],\n",
    "    \"democrat_house_members\": bill[\"democrat_house_members\"],\n",
    "    \"democrat_senate_members\": bill[\"democrat_senate_members\"],\n",
    "    \"republican_house_members\": bill[\"republican_house_members\"],\n",
    "    \"republican_senate_members\": bill[\"republican_senate_members\"],\n",
    "    \"house_incumbents\": bill[\"house_incumbents\"],\n",
    "    \"senate_incumbents\": bill[\"senate_incumbents\"],\n",
    "    \"house_freshman\": bill[\"house_freshman\"],\n",
    "    \"senate_freshman\": bill[\"senate_freshman\"],\n",
    "    \"house_members_age_under_30\": bill[\"house_members_age_under_30\"],\n",
    "    \"senate_members_age_under_30\": bill[\"senate_members_age_under_30\"],\n",
    "    \"house_members_age_30_to_39\": bill[\"house_members_age_30_to_39\"],\n",
    "    \"senate_members_age_30_to_39\": bill[\"senate_members_age_30_to_39\"],\n",
    "    \"house_members_age_40_to_49\": bill[\"house_members_age_40_to_49\"],\n",
    "    \"senate_members_age_40_to_49\": bill[\"senate_members_age_40_to_49\"],\n",
    "    \"house_members_age_50_to_59\": bill[\"house_members_age_50_to_59\"],\n",
    "    \"senate_members_age_50_to_59\": bill[\"senate_members_age_50_to_59\"],\n",
    "    \"house_members_age_60_to_69\": bill[\"house_members_age_60_to_69\"],\n",
    "    \"house_members_age_over_70\": bill[\"house_members_age_over_70\"],\n",
    "    \"senate_members_age_over_70\": bill[\"senate_members_age_over_70\"],\n",
    "    \"house_committee_democrats\": bill[\"house_committee_democrats\"],\n",
    "    \"house_committee_republicans\": bill[\"house_committee_republicans\"],\n",
    "    \"senate_committee_democrats\": bill[\"senate_committee_democrats\"],\n",
    "    \"senate_committee_republicans\": bill[\"senate_committee_republicans\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Committee Names Debacle\n",
    "\n",
    "If you read through the code you might be wondering about 'committee_dict.py', 'committee_mapping.py', and 'committee_names.txt'. These exist because in https://capitol.texas.gov and https://lrl.texas.gov the committee names are slightly different. For example the same committee might be called 'Appropriations - S/C on Budget Transparency and Reform' on one website, and 'Appropriations Subcommittee on Budget Transparency and Reform' on the other. My solution to this was to assign each committe a number, and then assign each committee name to the respective number. So 'Appropriations - S/C on Budget Transparency and Reform' and 'Appropriations Subcommittee on Budget Transparency and Reform' both represent committee 410.\n",
    "\n",
    "To make this mapping from the different websites I used the scripts and files 'committee_mapping.py', 'committee_names.txt', and 'commitee_dict.py'. You can read through those scripts and hopefully everything makes since.\n",
    "\n",
    "## The Overall Data\n",
    "\n",
    "Overall the data for each bill that was scraped using the above method are:\n",
    "- house_bill\n",
    "- senate_bill\n",
    "- bill_number\n",
    "- legislative_session\n",
    "- num_of_joint_authors\n",
    "- num_of_co_authors\n",
    "- num_of_subjects\n",
    "- passed\n",
    "- male_house_members\n",
    "- male_senate_members\n",
    "- female_house_members\n",
    "- female_senate_members\n",
    "- democrat_house_members\n",
    "- democrat_senate_members\n",
    "- republican_house_members\n",
    "- republican_senate_members\n",
    "- house_incumbents\n",
    "- senate_incumbents\n",
    "- house_freshman\n",
    "- senate_freshman\n",
    "- house_members_age_under_30\n",
    "- senate_members_age_under_30\n",
    "- house_members_age_30_to_39\n",
    "- senate_members_age_30_to_39\n",
    "- house_members_age_40_to_49\n",
    "- senate_members_age_40_to_49\n",
    "- house_members_age_50_to_59\n",
    "- senate_members_age_50_to_59\n",
    "- house_members_age_60_to_69\n",
    "- house_members_age_over_70\n",
    "- senate_members_age_over_70\n",
    "- house_committee_democrats\n",
    "- house_committee_republicans\n",
    "- senate_committee_democrats\n",
    "- senate_committee_republicans\n",
    "\n",
    "### Cleaning up the data and csv\n",
    "\n",
    "Because 'bill_data_scraper.py' can have multiple instances running at a time, the 'bill_data.csv' isn't organized in order. In order to sort this 'bill_data_cleanup.ipynb' was created. By running this script all that happens is it sorts the csv by name so it was easier to tell what data needed to be mined next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('bill_data.csv')\n",
    "\n",
    "df = df.sort_values(by=['legislative_session', 'senate_bill', 'bill_number'])\n",
    "df.to_csv('bill_data.csv', mode='w',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more script was used to clean up the data, 'removing_errors_from_data.ipynb'. There was an issue with house bills have senate committee data, and senate bills having house committee data. This data exists, but we don't want to use it because a bill with senate and house committee data must have passed at least one chamber of congress, meaning it is extremely likely to pass. The first model was way to accurate because of this, so 'removing_errors_from_data.ipynb' fixes this problem for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Now that we have all the data we just need to build the model. The model was built using the data in bill_data.csv. The model is built in 'building_the_fucking_model.ipynb'\n",
    "Some features need to be removed from the csv before we feed it into the model. Specifically\n",
    "- bill_number\n",
    "- legislative_session\n",
    "- house_bill\n",
    "- senate_bill\n",
    "- passed\n",
    "\n",
    "After we remove these columns we then split the data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('bill_data.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "X = df.copy()\n",
    "X.pop('bill_number')\n",
    "X.pop('legislative_session')\n",
    "X[\"house_bill\"] = X[\"house_bill\"].astype(int)\n",
    "X[\"senate_bill\"] = X[\"senate_bill\"].astype(int)\n",
    "X[\"passed\"] = X[\"passed\"].astype(int)\n",
    "y = X.pop('passed')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, train_size=0.75)\n",
    "input_shape = [X_train.shape[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make a simple deep learning model using tensorflow and keras. We use some batch normilization and dropout with early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.BatchNormalization(input_shape=input_shape),\n",
    "    \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(.3),\n",
    "    \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(.3),\n",
    "    \n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "model.save('./the_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "Then after the model is made there are two more code blocks in 'building_the_fucking_model.ipynb' used to evaluate the model.\n",
    "The code below is used to evaluate any specific bill in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "import pandas as pd\n",
    "testDF = pd.read_csv('bill_data.csv')\n",
    "T = testDF.copy()\n",
    "T[\"house_bill\"] = T[\"house_bill\"].astype(int)\n",
    "T[\"senate_bill\"] = T[\"senate_bill\"].astype(int)\n",
    "T[\"passed\"] = T[\"passed\"].astype(int)\n",
    "\n",
    "#legislative_session = \"82R\" #house_bill 472\n",
    "T = T.loc[(T['legislative_session'] == \"82R\") & (T['house_bill'] == 1) & (T['bill_number'] == 472)]\n",
    "T.pop('bill_number')\n",
    "T.pop('legislative_session')\n",
    "T.pop('passed')\n",
    "\n",
    "yhat = model.predict(T)\n",
    "print(yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to evaluate to model overall the following code makes a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE IS NOT MEANT TO BE RUN, IT IS JUST HERE FOR YOU TO READ AND UNDERSTAND WHAT IS GOING ON\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = model.predict(X_valid)\n",
    "y_pred = [1 if i > .5 else 0 for i in y_pred]\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the web app\n",
    "The web app uses a react front end and a python flask back end. To run the back end server you simply need to type 'python server.py' front the 'back_end' directory and it starts the server and reads the model from 'the_model' directory.\n",
    "In order to run the react front end you need to be in the 'front_end' directory and type 'npm start' in the terminal. The back_end wont work if you dont install the files in requirements.txt and the front end wont run if you dont have Node.js or the Node dependicies installed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d507929f44837d1daaf29f29c4c1ca0d68fa87396bd59872bf4a0b653e2f30d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
